INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 30
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: False
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): False
INFO:hybrid_inference:  - Memory Management (Quant): False
INFO:hybrid_inference:  - Edge-Cloud Partitioning: False (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Starting inference loop for 30 tokens...
INFO:hybrid_inference:
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 3.24s
INFO:hybrid_inference:Generation Loop Time: 2.45s
INFO:hybrid_inference:Average time per generated token: 0.0817s (12.24 tokens/s)
INFO:hybrid_inference:Generated 30 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 0
INFO:hybrid_inference:Tokens pruned (decision count): 0
INFO:hybrid_inference:Layers Offloaded: 0
INFO:hybrid_inference:Final sequence length: 45
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Generated: .

We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
