INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 100
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: False
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): False
INFO:hybrid_inference:  - Memory Management (Quant): True
INFO:hybrid_inference:  - Edge-Cloud Partitioning: False (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Initializing Memory Manager...
INFO:memory_manager:Memory Manager initialized with 10000MB budget, threshold 90%, Quantization: True
INFO:hybrid_inference:Starting inference loop for 100 tokens...
INFO:hybrid_inference:
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 22.21s
INFO:hybrid_inference:Generation Loop Time: 21.16s
INFO:hybrid_inference:Average time per generated token: 0.2115s (4.73 tokens/s)
INFO:hybrid_inference:Generated 100 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 0
INFO:hybrid_inference:Tokens Pruned: 0
INFO:hybrid_inference:Layers Offloaded: 0
INFO:hybrid_inference:Final sequence length: 115
INFO:hybrid_inference:--- Memory Manager Stats ---
INFO:hybrid_inference:Peak Memory Usage: 8.02 MB
INFO:hybrid_inference:Quantization Events: 0
INFO:hybrid_inference:Layers Quantized (0): []
INFO:hybrid_inference:Dequantization Events: 0
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Final Output: Test memory compression, layer skipping, and token pruning with pruning action.

We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is deployed and running, we will see our code run at its peak performance as we continue to push our client code to the network. In this instance, it was a huge boost since we ran our benchmark using 4GB of memory. This is a really big win for our tests because it makes it easier for us to test the process in real time.
INFO:hybrid_inference:Newly Generated Text: .  We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is deployed and running, we will see our code run at its peak performance as we continue to push our client code to the network. In this instance, it was a huge boost since we ran our benchmark using 4GB of memory. This is a really big win for our tests because it makes it easier for us to test the process in real time.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
INFO:hybrid_inference:Hybrid inference script finished.
