INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 20
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: False
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): False
INFO:hybrid_inference:  - Memory Management (Quant): True
INFO:hybrid_inference:  - Edge-Cloud Partitioning: False (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Initializing Memory Manager...
INFO:memory_manager:Memory Manager initialized with 10000MB budget, threshold 90%, Quantization: True
INFO:hybrid_inference:Starting inference loop for 20 tokens...
INFO:hybrid_inference:
=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 1.90s
INFO:hybrid_inference:Generation Loop Time: 1.36s
INFO:hybrid_inference:Average time per generated token: 0.0680s (14.71 tokens/s)
INFO:hybrid_inference:Generated 20 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 0
INFO:hybrid_inference:Tokens pruned (decision count): 0
INFO:hybrid_inference:Final sequence length: 35
INFO:hybrid_inference:--- Memory Manager Stats ---
INFO:hybrid_inference:Peak Memory Usage: 2.39 MB
INFO:hybrid_inference:Quantization Events: 0
INFO:hybrid_inference:Layers Quantized (0): []
INFO:hybrid_inference:Dequantization Events: 0
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Generated: .

We've used a tool called D3 to help us analyze and optimize memory usage while
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
