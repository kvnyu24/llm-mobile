INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 150
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: False
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): False
INFO:hybrid_inference:  - Memory Management (Quant): True
INFO:hybrid_inference:  - Edge-Cloud Partitioning: False (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Initializing Memory Manager...
INFO:memory_manager:Memory Manager initialized with 5MB budget, threshold 90%, Quantization: True
INFO:hybrid_inference:Starting inference loop for 150 tokens...
INFO:hybrid_inference:
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 48.31s
INFO:hybrid_inference:Generation Loop Time: 47.42s
INFO:hybrid_inference:Average time per generated token: 0.3160s (3.16 tokens/s)
INFO:hybrid_inference:Generated 150 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 0
INFO:hybrid_inference:Tokens Pruned: 0
INFO:hybrid_inference:Layers Offloaded: 0
INFO:hybrid_inference:Final sequence length: 165
INFO:hybrid_inference:--- Memory Manager Stats ---
INFO:hybrid_inference:Peak Memory Usage: 4.50 MB
INFO:hybrid_inference:Quantization Events: 690
INFO:hybrid_inference:Layers Quantized (10): [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:Dequantization Events: 680
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Generated: .

We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is deployed and running, we will see our code run at its peak performance as we continue to push our client code to the network. In this instance, it was a huge boost since we ran our benchmark using 4GB of memory. This is a really big win for our tests because it makes it easier for us to test the process in real time. We can now see that our test network is running at 4Gbps, making it more efficient for the testnet. And it's also a win because we can see what it is doing when we push a test to it. The performance of our
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
