INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 100
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: True
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): True
INFO:hybrid_inference:  - Memory Management (Quant): True
INFO:hybrid_inference:  - Edge-Cloud Partitioning: True (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Initializing Memory Manager...
INFO:memory_manager:Memory Manager initialized with 5MB budget, threshold 90%, Quantization: True
INFO:hybrid_inference:Initializing Token Pruner...
INFO:hybrid_inference:Initializing Layer Compression/Skipping Manager...
INFO:hybrid_inference:Initializing Edge-Cloud Manager...
INFO:edge_cloud_manager:EdgeCloudManager initialized for 12 layers.
INFO:hybrid_inference:Registering pre-forward hooks for layer skipping on 12 layers...
INFO:hybrid_inference:Successfully registered 12 hooks.
INFO:hybrid_inference:Starting inference loop for 100 tokens...
INFO:hybrid_inference:  Step 1 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
WARNING:token_pruning.token_pruner:Expected query length (Q) of 1 for scoring, but got 15. Using only the first query position.
INFO:hybrid_inference:  Step 2 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 3 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 4 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 5 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 6 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 7 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 8 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 9 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 10 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 11 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 20, 64]), Value=torch.Size([1, 12, 20, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 20, 64]), Value=torch.Size([1, 12, 20, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 23
INFO:hybrid_inference:  Step 12 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 20, 64]), Value=torch.Size([1, 12, 20, 64]), Index=torch.Size([22])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 20, 64]), Value=torch.Size([1, 12, 20, 64]), Index=torch.Size([22])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([22])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 22
INFO:hybrid_inference:  Step 13 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 14 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 15 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 16 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 17 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([24])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 24
INFO:hybrid_inference:  Step 18 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 19 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 20 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 21 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 22 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([27])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([27])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([27])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 27
INFO:hybrid_inference:  Step 23 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 3 tokens. New seq len: 25
INFO:hybrid_inference:  Step 24 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 6 tokens. New seq len: 20
INFO:hybrid_inference:  Step 25 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 20, 64]), Value=torch.Size([1, 12, 20, 64]), Index=torch.Size([21])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 21
INFO:hybrid_inference:  Step 26 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([22])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([22])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([22])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 22
INFO:hybrid_inference:  Step 27 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 21, 64]), Value=torch.Size([1, 12, 21, 64]), Index=torch.Size([22])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 22
INFO:hybrid_inference:  Step 28 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([21])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([21])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([21])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 21
INFO:hybrid_inference:  Step 29 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 30 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 22
INFO:hybrid_inference:  Step 31 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 32 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 33 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 34 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 25
INFO:hybrid_inference:  Step 35 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 36 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([26])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 26
INFO:hybrid_inference:  Step 37 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 38 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 39 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([27])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([27])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([27])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 27
INFO:hybrid_inference:  Step 40 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 41 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 42 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([28])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([28])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 28
INFO:hybrid_inference:  Step 43 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 3 tokens. New seq len: 26
INFO:hybrid_inference:  Step 44 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 45 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([27])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([27])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 27
INFO:hybrid_inference:  Step 46 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 47 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 48 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 49 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 29, 64]), Value=torch.Size([1, 12, 29, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 29, 64]), Value=torch.Size([1, 12, 29, 64]), Index=torch.Size([26])
INFO:token_pruning.token_pruner:Pruned 5 tokens. New seq len: 26
INFO:hybrid_inference:  Step 50 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 5 tokens. New seq len: 22
INFO:hybrid_inference:  Step 51 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 52 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 53 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 54 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 3 tokens. New seq len: 23
INFO:hybrid_inference:  Step 55 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 56 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 57 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 58 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 59 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 60 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 61 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 62 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([28])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([28])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([28])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 28
INFO:hybrid_inference:  Step 63 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([28])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 27, 64]), Value=torch.Size([1, 12, 27, 64]), Index=torch.Size([28])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 28
INFO:hybrid_inference:  Step 64 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
[DEBUG Temp Layer 2] Raw Temp: 0.4356 | Clamped: 0.4356 | MagNorm: 0.1257 (AvgMag: 1.2570) | VarNorm: 1.0000 (Var: 37.7082) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4886 | Clamped: 0.4886 | MagNorm: 0.1590 (AvgMag: 1.5903) | VarNorm: 1.0000 (Var: 580.7483) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5338 | Clamped: 0.5338 | MagNorm: 0.1766 (AvgMag: 1.7661) | VarNorm: 1.0000 (Var: 661.8407) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5785 | Clamped: 0.5785 | MagNorm: 0.1934 (AvgMag: 1.9344) | VarNorm: 1.0000 (Var: 734.1968) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4902 | Clamped: 0.4902 | MagNorm: 0.0896 (AvgMag: 0.8956) | VarNorm: 1.0000 (Var: 3.9703) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5317 | Clamped: 0.5317 | MagNorm: 0.0999 (AvgMag: 0.9986) | VarNorm: 1.0000 (Var: 3.7093) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4410 | Clamped: 0.4410 | MagNorm: 0.1365 (AvgMag: 1.3650) | VarNorm: 1.0000 (Var: 5.1746) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4828 | Clamped: 0.4828 | MagNorm: 0.1475 (AvgMag: 1.4746) | VarNorm: 1.0000 (Var: 5.7988) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5599 | Clamped: 0.5599 | MagNorm: 0.1561 (AvgMag: 1.5610) | VarNorm: 1.0000 (Var: 6.4412) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5058 | Clamped: 0.5058 | MagNorm: 0.1206 (AvgMag: 1.2059) | VarNorm: 1.0000 (Var: 4.1942) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5463 | Clamped: 0.5463 | MagNorm: 0.1289 (AvgMag: 1.2887) | VarNorm: 1.0000 (Var: 4.3879) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4309 | Clamped: 0.4309 | MagNorm: 0.1163 (AvgMag: 1.1629) | VarNorm: 1.0000 (Var: 4.2447) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4759 | Clamped: 0.4759 | MagNorm: 0.1336 (AvgMag: 1.3357) | VarNorm: 1.0000 (Var: 5.0113) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5208 | Clamped: 0.5208 | MagNorm: 0.1507 (AvgMag: 1.5067) | VarNorm: 1.0000 (Var: 5.7169) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5642 | Clamped: 0.5642 | MagNorm: 0.1647 (AvgMag: 1.6467) | VarNorm: 1.0000 (Var: 6.4977) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5072 | Clamped: 0.5072 | MagNorm: 0.1234 (AvgMag: 1.2345) | VarNorm: 1.0000 (Var: 4.2169) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5492 | Clamped: 0.5492 | MagNorm: 0.1348 (AvgMag: 1.3479) | VarNorm: 1.0000 (Var: 4.9653) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4325 | Clamped: 0.4325 | MagNorm: 0.1195 (AvgMag: 1.1955) | VarNorm: 1.0000 (Var: 4.2223) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4743 | Clamped: 0.4743 | MagNorm: 0.1304 (AvgMag: 1.3043) | VarNorm: 1.0000 (Var: 4.7417) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5215 | Clamped: 0.5215 | MagNorm: 0.1522 (AvgMag: 1.5215) | VarNorm: 1.0000 (Var: 6.1458) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5645 | Clamped: 0.5645 | MagNorm: 0.1653 (AvgMag: 1.6534) | VarNorm: 1.0000 (Var: 7.1862) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4934 | Clamped: 0.4934 | MagNorm: 0.0959 (AvgMag: 0.9594) | VarNorm: 1.0000 (Var: 3.9911) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5357 | Clamped: 0.5357 | MagNorm: 0.1078 (AvgMag: 1.0779) | VarNorm: 1.0000 (Var: 3.9761) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4343 | Clamped: 0.4343 | MagNorm: 0.1231 (AvgMag: 1.2310) | VarNorm: 1.0000 (Var: 5.1796) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4754 | Clamped: 0.4754 | MagNorm: 0.1326 (AvgMag: 1.3258) | VarNorm: 1.0000 (Var: 5.6059) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5541 | Clamped: 0.5541 | MagNorm: 0.1445 (AvgMag: 1.4449) | VarNorm: 1.0000 (Var: 5.7077) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4853 | Clamped: 0.4853 | MagNorm: 0.0796 (AvgMag: 0.7960) | VarNorm: 1.0000 (Var: 3.9367) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5232 | Clamped: 0.5232 | MagNorm: 0.0827 (AvgMag: 0.8271) | VarNorm: 1.0000 (Var: 3.3416) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4382 | Clamped: 0.4382 | MagNorm: 0.1309 (AvgMag: 1.3085) | VarNorm: 1.0000 (Var: 4.4015) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4811 | Clamped: 0.4811 | MagNorm: 0.1441 (AvgMag: 1.4409) | VarNorm: 1.0000 (Var: 4.9552) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5599 | Clamped: 0.5599 | MagNorm: 0.1563 (AvgMag: 1.5625) | VarNorm: 1.0000 (Var: 5.3740) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5138 | Clamped: 0.5138 | MagNorm: 0.1367 (AvgMag: 1.3666) | VarNorm: 1.0000 (Var: 5.0139) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5501 | Clamped: 0.5501 | MagNorm: 0.1365 (AvgMag: 1.3653) | VarNorm: 1.0000 (Var: 4.9020) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4290 | Clamped: 0.4290 | MagNorm: 0.1125 (AvgMag: 1.1251) | VarNorm: 1.0000 (Var: 4.2697) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4754 | Clamped: 0.4754 | MagNorm: 0.1327 (AvgMag: 1.3270) | VarNorm: 1.0000 (Var: 5.5795) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5182 | Clamped: 0.5182 | MagNorm: 0.1455 (AvgMag: 1.4549) | VarNorm: 1.0000 (Var: 6.3455) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5564 | Clamped: 0.5564 | MagNorm: 0.1492 (AvgMag: 1.4918) | VarNorm: 1.0000 (Var: 6.2585) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4998 | Clamped: 0.4998 | MagNorm: 0.1087 (AvgMag: 1.0866) | VarNorm: 1.0000 (Var: 4.3234) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5398 | Clamped: 0.5398 | MagNorm: 0.1159 (AvgMag: 1.1591) | VarNorm: 1.0000 (Var: 4.3449) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4155 | Clamped: 0.4155 | MagNorm: 0.0855 (AvgMag: 0.8550) | VarNorm: 1.0000 (Var: 3.9743) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4617 | Clamped: 0.4617 | MagNorm: 0.1052 (AvgMag: 1.0517) | VarNorm: 1.0000 (Var: 4.7898) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5404 | Clamped: 0.5404 | MagNorm: 0.1172 (AvgMag: 1.1723) | VarNorm: 1.0000 (Var: 5.2409) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4999 | Clamped: 0.4999 | MagNorm: 0.1089 (AvgMag: 1.0890) | VarNorm: 1.0000 (Var: 3.9628) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5405 | Clamped: 0.5405 | MagNorm: 0.1173 (AvgMag: 1.1730) | VarNorm: 1.0000 (Var: 3.9845) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4307 | Clamped: 0.4307 | MagNorm: 0.1159 (AvgMag: 1.1586) | VarNorm: 1.0000 (Var: 4.5405) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4722 | Clamped: 0.4722 | MagNorm: 0.1261 (AvgMag: 1.2613) | VarNorm: 1.0000 (Var: 5.1197) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5511 | Clamped: 0.5511 | MagNorm: 0.1386 (AvgMag: 1.3862) | VarNorm: 1.0000 (Var: 6.0486) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4871 | Clamped: 0.4871 | MagNorm: 0.0833 (AvgMag: 0.8325) | VarNorm: 1.0000 (Var: 4.0312) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5271 | Clamped: 0.5271 | MagNorm: 0.0906 (AvgMag: 0.9056) | VarNorm: 1.0000 (Var: 3.6733) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4318 | Clamped: 0.4318 | MagNorm: 0.1182 (AvgMag: 1.1817) | VarNorm: 1.0000 (Var: 4.2227) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4734 | Clamped: 0.4734 | MagNorm: 0.1286 (AvgMag: 1.2860) | VarNorm: 1.0000 (Var: 4.8162) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5517 | Clamped: 0.5517 | MagNorm: 0.1398 (AvgMag: 1.3976) | VarNorm: 1.0000 (Var: 5.2701) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5094 | Clamped: 0.5094 | MagNorm: 0.1279 (AvgMag: 1.2788) | VarNorm: 1.0000 (Var: 4.4444) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5483 | Clamped: 0.5483 | MagNorm: 0.1329 (AvgMag: 1.3288) | VarNorm: 1.0000 (Var: 4.6713) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4339 | Clamped: 0.4339 | MagNorm: 0.1223 (AvgMag: 1.2228) | VarNorm: 1.0000 (Var: 4.4138) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4738 | Clamped: 0.4738 | MagNorm: 0.1295 (AvgMag: 1.2952) | VarNorm: 1.0000 (Var: 4.5240) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5200 | Clamped: 0.5200 | MagNorm: 0.1491 (AvgMag: 1.4908) | VarNorm: 1.0000 (Var: 5.3729) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5628 | Clamped: 0.5628 | MagNorm: 0.1619 (AvgMag: 1.6194) | VarNorm: 1.0000 (Var: 6.4063) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4823 | Clamped: 0.4823 | MagNorm: 0.0736 (AvgMag: 0.7365) | VarNorm: 1.0000 (Var: 3.8242) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5228 | Clamped: 0.5228 | MagNorm: 0.0819 (AvgMag: 0.8193) | VarNorm: 1.0000 (Var: 3.4192) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4312 | Clamped: 0.4312 | MagNorm: 0.1170 (AvgMag: 1.1704) | VarNorm: 1.0000 (Var: 4.4212) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4765 | Clamped: 0.4765 | MagNorm: 0.1347 (AvgMag: 1.3473) | VarNorm: 1.0000 (Var: 4.7628) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5542 | Clamped: 0.5542 | MagNorm: 0.1447 (AvgMag: 1.4470) | VarNorm: 1.0000 (Var: 5.0601) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4979 | Clamped: 0.4979 | MagNorm: 0.1049 (AvgMag: 1.0490) | VarNorm: 1.0000 (Var: 3.9850) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5370 | Clamped: 0.5370 | MagNorm: 0.1104 (AvgMag: 1.1043) | VarNorm: 1.0000 (Var: 3.9835) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4253 | Clamped: 0.4253 | MagNorm: 0.1052 (AvgMag: 1.0521) | VarNorm: 1.0000 (Var: 3.9964) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4686 | Clamped: 0.4686 | MagNorm: 0.1191 (AvgMag: 1.1908) | VarNorm: 1.0000 (Var: 4.1610) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5484 | Clamped: 0.5484 | MagNorm: 0.1332 (AvgMag: 1.3321) | VarNorm: 1.0000 (Var: 4.7990) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4980 | Clamped: 0.4980 | MagNorm: 0.1050 (AvgMag: 1.0500) | VarNorm: 1.0000 (Var: 4.1307) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5359 | Clamped: 0.5359 | MagNorm: 0.1082 (AvgMag: 1.0820) | VarNorm: 1.0000 (Var: 3.8082) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4238 | Clamped: 0.4238 | MagNorm: 0.1022 (AvgMag: 1.0218) | VarNorm: 1.0000 (Var: 3.3608) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4674 | Clamped: 0.4674 | MagNorm: 0.1165 (AvgMag: 1.1652) | VarNorm: 1.0000 (Var: 3.7439) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5440 | Clamped: 0.5440 | MagNorm: 0.1243 (AvgMag: 1.2426) | VarNorm: 1.0000 (Var: 4.1383) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5082 | Clamped: 0.5082 | MagNorm: 0.1254 (AvgMag: 1.2542) | VarNorm: 1.0000 (Var: 4.9430) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5473 | Clamped: 0.5473 | MagNorm: 0.1309 (AvgMag: 1.3095) | VarNorm: 1.0000 (Var: 5.0184) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4336 | Clamped: 0.4336 | MagNorm: 0.1217 (AvgMag: 1.2170) | VarNorm: 1.0000 (Var: 4.9298) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4723 | Clamped: 0.4723 | MagNorm: 0.1264 (AvgMag: 1.2644) | VarNorm: 1.0000 (Var: 4.8648) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5129 | Clamped: 0.5129 | MagNorm: 0.1348 (AvgMag: 1.3482) | VarNorm: 1.0000 (Var: 5.3450) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5552 | Clamped: 0.5552 | MagNorm: 0.1467 (AvgMag: 1.4673) | VarNorm: 1.0000 (Var: 5.6452) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5108 | Clamped: 0.5108 | MagNorm: 0.1306 (AvgMag: 1.3062) | VarNorm: 1.0000 (Var: 4.7694) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5521 | Clamped: 0.5521 | MagNorm: 0.1406 (AvgMag: 1.4062) | VarNorm: 1.0000 (Var: 5.0318) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4356 | Clamped: 0.4356 | MagNorm: 0.1257 (AvgMag: 1.2573) | VarNorm: 1.0000 (Var: 4.7901) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4761 | Clamped: 0.4761 | MagNorm: 0.1341 (AvgMag: 1.3408) | VarNorm: 1.0000 (Var: 5.0927) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5205 | Clamped: 0.5205 | MagNorm: 0.1501 (AvgMag: 1.5014) | VarNorm: 1.0000 (Var: 6.1071) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5609 | Clamped: 0.5609 | MagNorm: 0.1581 (AvgMag: 1.5812) | VarNorm: 1.0000 (Var: 6.5277) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4921 | Clamped: 0.4921 | MagNorm: 0.0933 (AvgMag: 0.9327) | VarNorm: 1.0000 (Var: 3.6341) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5332 | Clamped: 0.5332 | MagNorm: 0.1027 (AvgMag: 1.0271) | VarNorm: 1.0000 (Var: 3.6485) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4238 | Clamped: 0.4238 | MagNorm: 0.1022 (AvgMag: 1.0217) | VarNorm: 1.0000 (Var: 3.8909) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4644 | Clamped: 0.4644 | MagNorm: 0.1107 (AvgMag: 1.1071) | VarNorm: 1.0000 (Var: 4.2107) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5460 | Clamped: 0.5460 | MagNorm: 0.1283 (AvgMag: 1.2830) | VarNorm: 1.0000 (Var: 5.7663) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4951 | Clamped: 0.4951 | MagNorm: 0.0992 (AvgMag: 0.9921) | VarNorm: 1.0000 (Var: 3.4908) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5369 | Clamped: 0.5369 | MagNorm: 0.1102 (AvgMag: 1.1019) | VarNorm: 1.0000 (Var: 4.0978) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4223 | Clamped: 0.4223 | MagNorm: 0.0991 (AvgMag: 0.9911) | VarNorm: 1.0000 (Var: 3.5253) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4649 | Clamped: 0.4649 | MagNorm: 0.1117 (AvgMag: 1.1167) | VarNorm: 1.0000 (Var: 3.7860) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5453 | Clamped: 0.5453 | MagNorm: 0.1270 (AvgMag: 1.2701) | VarNorm: 1.0000 (Var: 4.4030) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5026 | Clamped: 0.5026 | MagNorm: 0.1142 (AvgMag: 1.1423) | VarNorm: 1.0000 (Var: 4.1885) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5432 | Clamped: 0.5432 | MagNorm: 0.1228 (AvgMag: 1.2277) | VarNorm: 1.0000 (Var: 4.4798) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4185 | Clamped: 0.4185 | MagNorm: 0.0915 (AvgMag: 0.9153) | VarNorm: 1.0000 (Var: 3.7246) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4590 | Clamped: 0.4590 | MagNorm: 0.0999 (AvgMag: 0.9991) | VarNorm: 1.0000 (Var: 3.7151) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.4991 | Clamped: 0.4991 | MagNorm: 0.1073 (AvgMag: 1.0734) | VarNorm: 1.0000 (Var: 3.9479) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5409 | Clamped: 0.5409 | MagNorm: 0.1181 (AvgMag: 1.1814) | VarNorm: 1.0000 (Var: 4.6233) | PosF: 0.9091
[DEBUG Temp Layer 5] Raw Temp: 0.5444 | Clamped: 0.5444 | MagNorm: 0.1251 (AvgMag: 1.2514) | VarNorm: 1.0000 (Var: 4.9517) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4348 | Clamped: 0.4348 | MagNorm: 0.1242 (AvgMag: 1.2422) | VarNorm: 1.0000 (Var: 5.0938) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4757 | Clamped: 0.4757 | MagNorm: 0.1333 (AvgMag: 1.3331) | VarNorm: 1.0000 (Var: 5.5143) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5186 | Clamped: 0.5186 | MagNorm: 0.1463 (AvgMag: 1.4626) | VarNorm: 1.0000 (Var: 6.2422) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5609 | Clamped: 0.5609 | MagNorm: 0.1581 (AvgMag: 1.5809) | VarNorm: 1.0000 (Var: 6.6333) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5100 | Clamped: 0.5100 | MagNorm: 0.1292 (AvgMag: 1.2916) | VarNorm: 1.0000 (Var: 5.2146) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5512 | Clamped: 0.5512 | MagNorm: 0.1387 (AvgMag: 1.3872) | VarNorm: 1.0000 (Var: 5.7054) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4351 | Clamped: 0.4351 | MagNorm: 0.1248 (AvgMag: 1.2484) | VarNorm: 1.0000 (Var: 5.1483) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4813 | Clamped: 0.4813 | MagNorm: 0.1444 (AvgMag: 1.4439) | VarNorm: 1.0000 (Var: 6.1780) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5269 | Clamped: 0.5269 | MagNorm: 0.1629 (AvgMag: 1.6294) | VarNorm: 1.0000 (Var: 7.1852) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5693 | Clamped: 0.5693 | MagNorm: 0.1750 (AvgMag: 1.7495) | VarNorm: 1.0000 (Var: 7.9584) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5063 | Clamped: 0.5063 | MagNorm: 0.1217 (AvgMag: 1.2175) | VarNorm: 1.0000 (Var: 4.2742) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5473 | Clamped: 0.5473 | MagNorm: 0.1310 (AvgMag: 1.3102) | VarNorm: 1.0000 (Var: 4.5775) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4411 | Clamped: 0.4411 | MagNorm: 0.1368 (AvgMag: 1.3678) | VarNorm: 1.0000 (Var: 4.9192) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4877 | Clamped: 0.4877 | MagNorm: 0.1572 (AvgMag: 1.5719) | VarNorm: 1.0000 (Var: 5.8504) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5317 | Clamped: 0.5317 | MagNorm: 0.1725 (AvgMag: 1.7249) | VarNorm: 1.0000 (Var: 6.7466) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5734 | Clamped: 0.5734 | MagNorm: 0.1832 (AvgMag: 1.8318) | VarNorm: 1.0000 (Var: 7.4724) | PosF: 0.9091
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 65 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 66 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([29])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 28, 64]), Value=torch.Size([1, 12, 28, 64]), Index=torch.Size([29])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 29
INFO:hybrid_inference:  Step 67 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 29, 64]), Value=torch.Size([1, 12, 29, 64]), Index=torch.Size([29])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 29, 64]), Value=torch.Size([1, 12, 29, 64]), Index=torch.Size([29])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 29, 64]), Value=torch.Size([1, 12, 29, 64]), Index=torch.Size([29])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 29
INFO:hybrid_inference:  Step 68 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 69 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 30, 64]), Value=torch.Size([1, 12, 30, 64]), Index=torch.Size([29])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 30, 64]), Value=torch.Size([1, 12, 30, 64]), Index=torch.Size([29])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 29
INFO:hybrid_inference:  Step 70 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 71 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 30
INFO:hybrid_inference:  Step 72 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 73 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:  Step 74 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 31, 64]), Value=torch.Size([1, 12, 31, 64]), Index=torch.Size([29])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 31, 64]), Value=torch.Size([1, 12, 31, 64]), Index=torch.Size([29])
INFO:token_pruning.token_pruner:Pruned 4 tokens. New seq len: 29
INFO:hybrid_inference:  Step 75 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 7 tokens. New seq len: 23
INFO:hybrid_inference:  Step 76 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 77 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 78 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 24
INFO:hybrid_inference:  Step 79 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([24])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 24
INFO:hybrid_inference:  Step 80 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 25
INFO:hybrid_inference:  Step 81 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 25
INFO:hybrid_inference:  Step 82 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 26
INFO:hybrid_inference:  Step 83 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([24])
INFO:token_pruning.token_pruner:Pruned 3 tokens. New seq len: 24
INFO:hybrid_inference:  Step 84 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 25
INFO:hybrid_inference:  Step 85 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 24
INFO:hybrid_inference:  Step 86 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 25
INFO:hybrid_inference:  Step 87 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 25
INFO:hybrid_inference:  Step 88 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 26
INFO:hybrid_inference:  Step 89 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 26, 64]), Value=torch.Size([1, 12, 26, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 25
INFO:hybrid_inference:  Step 90 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 26
INFO:hybrid_inference:  Step 91 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 5 tokens. New seq len: 22
INFO:hybrid_inference:  Step 92 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 22
INFO:hybrid_inference:  Step 93 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 22
INFO:hybrid_inference:  Step 94 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 23
INFO:hybrid_inference:  Step 95 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:token_pruning.token_pruner:Pruned 2 tokens. New seq len: 22
INFO:hybrid_inference:  Step 96 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 22, 64]), Value=torch.Size([1, 12, 22, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 23
INFO:hybrid_inference:  Step 97 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([23])
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
INFO:hybrid_inference:  Step 98 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 23, 64]), Value=torch.Size([1, 12, 23, 64]), Index=torch.Size([24])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 24
INFO:hybrid_inference:  Step 99 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([25])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 25
INFO:hybrid_inference:  Step 100 Partition: Local=[0, 1, 2, 3, 4, 5], Remote=[6, 7, 8, 9, 10, 11]
INFO:hybrid_inference:    [Offload Check] Layer 6 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 7 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 8 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 9 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 10 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
INFO:hybrid_inference:    [Offload Check] Layer 11 is in remote_layers: [6, 7, 8, 9, 10, 11]. Incrementing counter.
ERROR:token_pruning.token_pruner:Error during index_select for layer 2: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 3: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 24, 64]), Value=torch.Size([1, 12, 24, 64]), Index=torch.Size([26])
ERROR:token_pruning.token_pruner:Error during index_select for layer 4: index out of range in self
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/token_pruning/token_pruner.py", line 317, in prune_state
    pruned_keys = torch.index_select(layer_keys, dim=self.kv_cache_dim, index=keep_indices_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: index out of range in self
ERROR:token_pruning.token_pruner:  Shapes: Key=torch.Size([1, 12, 25, 64]), Value=torch.Size([1, 12, 25, 64]), Index=torch.Size([26])
INFO:token_pruning.token_pruner:Pruned 0 tokens. New seq len: 26
INFO:hybrid_inference:Removing 12 hooks...
INFO:hybrid_inference:Hooks removed.
INFO:hybrid_inference:
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 25.16s
INFO:hybrid_inference:Generation Loop Time: 23.65s
INFO:hybrid_inference:Average time per generated token: 0.2364s (4.23 tokens/s)
INFO:hybrid_inference:Generated 100 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 120
INFO:hybrid_inference:Tokens Pruned: 88
INFO:hybrid_inference:Layers Offloaded: 600
INFO:hybrid_inference:Final sequence length: 27
INFO:hybrid_inference:--- Memory Manager Stats ---
INFO:hybrid_inference:Peak Memory Usage: 1.11 MB
INFO:hybrid_inference:Quantization Events: 0
INFO:hybrid_inference:Layers Quantized (0): []
INFO:hybrid_inference:Dequantization Events: 0
INFO:hybrid_inference:--- Layer Optimization Stats ---
INFO:hybrid_inference:Compression Ratio (Handler): 0.0
INFO:hybrid_inference:Total Layers Evaluated for Skipping (Handler): 280
INFO:hybrid_inference:Total Layers Skipped (Handler): 120
INFO:hybrid_inference:Skipping Efficiency (Handler): 0.4332
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Final Output: Test memory compression, layer skipping, and token pruning with pruning action on behalf side sidewisewise (including the same exact amount amount of workflows and thereforeforthforth. Howevercherschercherrystalstalumnumnskinshipyieldsaurusaurusensisensisaeominensisiiensis Species Created Created Workshopshopefully illuminous amounts amounts amount amounts quantities quantities quantity quantity quantities of the entire lifetimes ago ago (orificorsor equivalent equivalent) -_-_-minutes ago)

               c2ndo) – ?? ?? ?
INFO:hybrid_inference:Newly Generated Text: on behalf side sidewisewise (including the same exact amount amount of workflows and thereforeforthforth. Howevercherschercherrystalstalumnumnskinshipyieldsaurusaurusensisensisaeominensisiiensis Species Created Created Workshopshopefully illuminous amounts amounts amount amounts quantities quantities quantity quantity quantities of the entire lifetimes ago ago (orificorsor equivalent equivalent) -_-_-minutes ago)                 c2ndo) – ?? ?? ?
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
INFO:hybrid_inference:Hybrid inference script finished.
[DEBUG Temp Layer 4] Raw Temp: 0.5077 | Clamped: 0.5077 | MagNorm: 0.1245 (AvgMag: 1.2452) | VarNorm: 1.0000 (Var: 5.1670) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5486 | Clamped: 0.5486 | MagNorm: 0.1335 (AvgMag: 1.3347) | VarNorm: 1.0000 (Var: 5.5084) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4342 | Clamped: 0.4342 | MagNorm: 0.1230 (AvgMag: 1.2299) | VarNorm: 1.0000 (Var: 4.6256) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4830 | Clamped: 0.4830 | MagNorm: 0.1477 (AvgMag: 1.4772) | VarNorm: 1.0000 (Var: 5.9441) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5288 | Clamped: 0.5288 | MagNorm: 0.1666 (AvgMag: 1.6662) | VarNorm: 1.0000 (Var: 6.7634) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5741 | Clamped: 0.5741 | MagNorm: 0.1846 (AvgMag: 1.8459) | VarNorm: 1.0000 (Var: 7.9382) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5078 | Clamped: 0.5078 | MagNorm: 0.1247 (AvgMag: 1.2474) | VarNorm: 1.0000 (Var: 5.2087) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5488 | Clamped: 0.5488 | MagNorm: 0.1339 (AvgMag: 1.3394) | VarNorm: 1.0000 (Var: 5.5339) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4410 | Clamped: 0.4410 | MagNorm: 0.1366 (AvgMag: 1.3660) | VarNorm: 1.0000 (Var: 5.7867) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4852 | Clamped: 0.4852 | MagNorm: 0.1523 (AvgMag: 1.5225) | VarNorm: 1.0000 (Var: 6.9089) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5278 | Clamped: 0.5278 | MagNorm: 0.1648 (AvgMag: 1.6475) | VarNorm: 1.0000 (Var: 7.2793) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5733 | Clamped: 0.5733 | MagNorm: 0.1830 (AvgMag: 1.8298) | VarNorm: 1.0000 (Var: 8.8932) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5055 | Clamped: 0.5055 | MagNorm: 0.1202 (AvgMag: 1.2018) | VarNorm: 1.0000 (Var: 4.9006) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5468 | Clamped: 0.5468 | MagNorm: 0.1300 (AvgMag: 1.2995) | VarNorm: 1.0000 (Var: 4.8999) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4316 | Clamped: 0.4316 | MagNorm: 0.1178 (AvgMag: 1.1780) | VarNorm: 1.0000 (Var: 4.8960) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4739 | Clamped: 0.4739 | MagNorm: 0.1297 (AvgMag: 1.2971) | VarNorm: 1.0000 (Var: 5.3497) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5196 | Clamped: 0.5196 | MagNorm: 0.1484 (AvgMag: 1.4836) | VarNorm: 1.0000 (Var: 6.5295) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5643 | Clamped: 0.5643 | MagNorm: 0.1649 (AvgMag: 1.6494) | VarNorm: 1.0000 (Var: 7.9841) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5034 | Clamped: 0.5034 | MagNorm: 0.1159 (AvgMag: 1.1589) | VarNorm: 1.0000 (Var: 4.5688) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5471 | Clamped: 0.5471 | MagNorm: 0.1305 (AvgMag: 1.3050) | VarNorm: 1.0000 (Var: 5.1269) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4277 | Clamped: 0.4277 | MagNorm: 0.1100 (AvgMag: 1.0995) | VarNorm: 1.0000 (Var: 4.1279) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4707 | Clamped: 0.4707 | MagNorm: 0.1232 (AvgMag: 1.2317) | VarNorm: 1.0000 (Var: 5.1483) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5166 | Clamped: 0.5166 | MagNorm: 0.1422 (AvgMag: 1.4219) | VarNorm: 1.0000 (Var: 6.5049) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5608 | Clamped: 0.5608 | MagNorm: 0.1580 (AvgMag: 1.5802) | VarNorm: 1.0000 (Var: 7.4169) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5056 | Clamped: 0.5056 | MagNorm: 0.1203 (AvgMag: 1.2032) | VarNorm: 1.0000 (Var: 4.1332) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5521 | Clamped: 0.5521 | MagNorm: 0.1406 (AvgMag: 1.4064) | VarNorm: 1.0000 (Var: 5.7414) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4354 | Clamped: 0.4354 | MagNorm: 0.1253 (AvgMag: 1.2532) | VarNorm: 1.0000 (Var: 4.7849) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4790 | Clamped: 0.4790 | MagNorm: 0.1397 (AvgMag: 1.3974) | VarNorm: 1.0000 (Var: 5.5402) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5280 | Clamped: 0.5280 | MagNorm: 0.1651 (AvgMag: 1.6507) | VarNorm: 1.0000 (Var: 7.4366) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5696 | Clamped: 0.5696 | MagNorm: 0.1755 (AvgMag: 1.7553) | VarNorm: 1.0000 (Var: 8.4553) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5089 | Clamped: 0.5089 | MagNorm: 0.1270 (AvgMag: 1.2698) | VarNorm: 1.0000 (Var: 4.4276) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5530 | Clamped: 0.5530 | MagNorm: 0.1424 (AvgMag: 1.4243) | VarNorm: 1.0000 (Var: 5.2515) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4425 | Clamped: 0.4425 | MagNorm: 0.1395 (AvgMag: 1.3949) | VarNorm: 1.0000 (Var: 5.1357) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4827 | Clamped: 0.4827 | MagNorm: 0.1472 (AvgMag: 1.4724) | VarNorm: 1.0000 (Var: 5.4134) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5265 | Clamped: 0.5265 | MagNorm: 0.1621 (AvgMag: 1.6209) | VarNorm: 1.0000 (Var: 6.0750) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5705 | Clamped: 0.5705 | MagNorm: 0.1774 (AvgMag: 1.7737) | VarNorm: 1.0000 (Var: 7.1713) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4950 | Clamped: 0.4950 | MagNorm: 0.0992 (AvgMag: 0.9919) | VarNorm: 1.0000 (Var: 3.9348) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5397 | Clamped: 0.5397 | MagNorm: 0.1158 (AvgMag: 1.1582) | VarNorm: 1.0000 (Var: 4.3961) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4303 | Clamped: 0.4303 | MagNorm: 0.1151 (AvgMag: 1.1508) | VarNorm: 1.0000 (Var: 4.2902) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4700 | Clamped: 0.4700 | MagNorm: 0.1218 (AvgMag: 1.2180) | VarNorm: 1.0000 (Var: 4.4980) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5513 | Clamped: 0.5513 | MagNorm: 0.1389 (AvgMag: 1.3892) | VarNorm: 1.0000 (Var: 5.3201) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5025 | Clamped: 0.5025 | MagNorm: 0.1141 (AvgMag: 1.1414) | VarNorm: 1.0000 (Var: 4.3511) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5446 | Clamped: 0.5446 | MagNorm: 0.1255 (AvgMag: 1.2552) | VarNorm: 1.0000 (Var: 4.6966) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4286 | Clamped: 0.4286 | MagNorm: 0.1117 (AvgMag: 1.1173) | VarNorm: 1.0000 (Var: 4.3441) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4672 | Clamped: 0.4672 | MagNorm: 0.1163 (AvgMag: 1.1626) | VarNorm: 1.0000 (Var: 4.6196) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5108 | Clamped: 0.5108 | MagNorm: 0.1308 (AvgMag: 1.3077) | VarNorm: 1.0000 (Var: 5.2173) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5532 | Clamped: 0.5532 | MagNorm: 0.1427 (AvgMag: 1.4272) | VarNorm: 1.0000 (Var: 6.0035) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5019 | Clamped: 0.5019 | MagNorm: 0.1129 (AvgMag: 1.1294) | VarNorm: 1.0000 (Var: 4.4573) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5431 | Clamped: 0.5431 | MagNorm: 0.1225 (AvgMag: 1.2248) | VarNorm: 1.0000 (Var: 4.6610) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4319 | Clamped: 0.4319 | MagNorm: 0.1184 (AvgMag: 1.1840) | VarNorm: 1.0000 (Var: 4.8311) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4712 | Clamped: 0.4712 | MagNorm: 0.1242 (AvgMag: 1.2415) | VarNorm: 1.0000 (Var: 5.1441) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5122 | Clamped: 0.5122 | MagNorm: 0.1334 (AvgMag: 1.3343) | VarNorm: 1.0000 (Var: 5.3542) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5560 | Clamped: 0.5560 | MagNorm: 0.1484 (AvgMag: 1.4844) | VarNorm: 1.0000 (Var: 6.2246) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5055 | Clamped: 0.5055 | MagNorm: 0.1200 (AvgMag: 1.2004) | VarNorm: 1.0000 (Var: 4.9566) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5472 | Clamped: 0.5472 | MagNorm: 0.1307 (AvgMag: 1.3067) | VarNorm: 1.0000 (Var: 5.3912) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4377 | Clamped: 0.4377 | MagNorm: 0.1300 (AvgMag: 1.3003) | VarNorm: 1.0000 (Var: 5.3452) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4786 | Clamped: 0.4786 | MagNorm: 0.1391 (AvgMag: 1.3911) | VarNorm: 1.0000 (Var: 5.9941) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5217 | Clamped: 0.5217 | MagNorm: 0.1525 (AvgMag: 1.5247) | VarNorm: 1.0000 (Var: 6.6058) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5635 | Clamped: 0.5635 | MagNorm: 0.1633 (AvgMag: 1.6329) | VarNorm: 1.0000 (Var: 6.9987) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5104 | Clamped: 0.5104 | MagNorm: 0.1299 (AvgMag: 1.2992) | VarNorm: 1.0000 (Var: 5.3415) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5519 | Clamped: 0.5519 | MagNorm: 0.1402 (AvgMag: 1.4020) | VarNorm: 1.0000 (Var: 5.7888) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4333 | Clamped: 0.4333 | MagNorm: 0.1211 (AvgMag: 1.2112) | VarNorm: 1.0000 (Var: 5.0247) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4752 | Clamped: 0.4752 | MagNorm: 0.1322 (AvgMag: 1.3216) | VarNorm: 1.0000 (Var: 5.7957) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5187 | Clamped: 0.5187 | MagNorm: 0.1465 (AvgMag: 1.4648) | VarNorm: 1.0000 (Var: 6.5219) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5600 | Clamped: 0.5600 | MagNorm: 0.1564 (AvgMag: 1.5637) | VarNorm: 1.0000 (Var: 6.7117) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4887 | Clamped: 0.4887 | MagNorm: 0.0866 (AvgMag: 0.8656) | VarNorm: 1.0000 (Var: 4.2108) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5298 | Clamped: 0.5298 | MagNorm: 0.0959 (AvgMag: 0.9594) | VarNorm: 1.0000 (Var: 4.2409) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4126 | Clamped: 0.4126 | MagNorm: 0.0797 (AvgMag: 0.7968) | VarNorm: 1.0000 (Var: 4.2072) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4567 | Clamped: 0.4567 | MagNorm: 0.0952 (AvgMag: 0.9522) | VarNorm: 1.0000 (Var: 5.0271) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5364 | Clamped: 0.5364 | MagNorm: 0.1092 (AvgMag: 1.0924) | VarNorm: 1.0000 (Var: 5.0982) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5070 | Clamped: 0.5070 | MagNorm: 0.1231 (AvgMag: 1.2309) | VarNorm: 1.0000 (Var: 4.8562) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5460 | Clamped: 0.5460 | MagNorm: 0.1285 (AvgMag: 1.2846) | VarNorm: 1.0000 (Var: 5.1507) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4473 | Clamped: 0.4473 | MagNorm: 0.1492 (AvgMag: 1.4918) | VarNorm: 1.0000 (Var: 6.1944) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4895 | Clamped: 0.4895 | MagNorm: 0.1608 (AvgMag: 1.6083) | VarNorm: 1.0000 (Var: 6.8542) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5338 | Clamped: 0.5338 | MagNorm: 0.1768 (AvgMag: 1.7675) | VarNorm: 1.0000 (Var: 7.4541) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5756 | Clamped: 0.5756 | MagNorm: 0.1875 (AvgMag: 1.8753) | VarNorm: 1.0000 (Var: 8.3288) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5045 | Clamped: 0.5045 | MagNorm: 0.1181 (AvgMag: 1.1813) | VarNorm: 1.0000 (Var: 4.9827) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5483 | Clamped: 0.5483 | MagNorm: 0.1330 (AvgMag: 1.3297) | VarNorm: 1.0000 (Var: 5.9394) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4302 | Clamped: 0.4302 | MagNorm: 0.1150 (AvgMag: 1.1499) | VarNorm: 1.0000 (Var: 4.1068) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4719 | Clamped: 0.4719 | MagNorm: 0.1257 (AvgMag: 1.2567) | VarNorm: 1.0000 (Var: 4.6660) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5138 | Clamped: 0.5138 | MagNorm: 0.1368 (AvgMag: 1.3676) | VarNorm: 1.0000 (Var: 5.0440) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5566 | Clamped: 0.5566 | MagNorm: 0.1496 (AvgMag: 1.4956) | VarNorm: 1.0000 (Var: 6.0412) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5021 | Clamped: 0.5021 | MagNorm: 0.1133 (AvgMag: 1.1329) | VarNorm: 1.0000 (Var: 4.2062) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5442 | Clamped: 0.5442 | MagNorm: 0.1248 (AvgMag: 1.2482) | VarNorm: 1.0000 (Var: 4.6246) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4218 | Clamped: 0.4218 | MagNorm: 0.0982 (AvgMag: 0.9823) | VarNorm: 1.0000 (Var: 4.2877) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4610 | Clamped: 0.4610 | MagNorm: 0.1038 (AvgMag: 1.0376) | VarNorm: 1.0000 (Var: 4.6557) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5025 | Clamped: 0.5025 | MagNorm: 0.1141 (AvgMag: 1.1405) | VarNorm: 1.0000 (Var: 4.6868) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5463 | Clamped: 0.5463 | MagNorm: 0.1290 (AvgMag: 1.2904) | VarNorm: 1.0000 (Var: 5.6841) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5032 | Clamped: 0.5032 | MagNorm: 0.1156 (AvgMag: 1.1557) | VarNorm: 1.0000 (Var: 4.3444) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5430 | Clamped: 0.5430 | MagNorm: 0.1224 (AvgMag: 1.2237) | VarNorm: 1.0000 (Var: 4.3293) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4403 | Clamped: 0.4403 | MagNorm: 0.1352 (AvgMag: 1.3518) | VarNorm: 1.0000 (Var: 4.6592) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4830 | Clamped: 0.4830 | MagNorm: 0.1477 (AvgMag: 1.4772) | VarNorm: 1.0000 (Var: 5.0839) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5273 | Clamped: 0.5273 | MagNorm: 0.1638 (AvgMag: 1.6376) | VarNorm: 1.0000 (Var: 5.6987) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5705 | Clamped: 0.5705 | MagNorm: 0.1774 (AvgMag: 1.7744) | VarNorm: 1.0000 (Var: 6.5022) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4989 | Clamped: 0.4989 | MagNorm: 0.1069 (AvgMag: 1.0691) | VarNorm: 1.0000 (Var: 3.9537) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5414 | Clamped: 0.5414 | MagNorm: 0.1191 (AvgMag: 1.1912) | VarNorm: 1.0000 (Var: 4.2786) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4298 | Clamped: 0.4298 | MagNorm: 0.1142 (AvgMag: 1.1423) | VarNorm: 1.0000 (Var: 4.4568) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4703 | Clamped: 0.4703 | MagNorm: 0.1223 (AvgMag: 1.2235) | VarNorm: 1.0000 (Var: 4.6846) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5500 | Clamped: 0.5500 | MagNorm: 0.1364 (AvgMag: 1.3642) | VarNorm: 1.0000 (Var: 5.2078) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5122 | Clamped: 0.5122 | MagNorm: 0.1335 (AvgMag: 1.3352) | VarNorm: 1.0000 (Var: 4.9082) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5527 | Clamped: 0.5527 | MagNorm: 0.1418 (AvgMag: 1.4175) | VarNorm: 1.0000 (Var: 5.1706) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4397 | Clamped: 0.4397 | MagNorm: 0.1340 (AvgMag: 1.3395) | VarNorm: 1.0000 (Var: 5.0060) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4800 | Clamped: 0.4800 | MagNorm: 0.1418 (AvgMag: 1.4178) | VarNorm: 1.0000 (Var: 5.2257) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5279 | Clamped: 0.5279 | MagNorm: 0.1649 (AvgMag: 1.6494) | VarNorm: 1.0000 (Var: 6.3679) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5713 | Clamped: 0.5713 | MagNorm: 0.1789 (AvgMag: 1.7888) | VarNorm: 1.0000 (Var: 7.6037) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4947 | Clamped: 0.4947 | MagNorm: 0.0985 (AvgMag: 0.9854) | VarNorm: 1.0000 (Var: 4.1258) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5367 | Clamped: 0.5367 | MagNorm: 0.1097 (AvgMag: 1.0971) | VarNorm: 1.0000 (Var: 4.2297) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4240 | Clamped: 0.4240 | MagNorm: 0.1025 (AvgMag: 1.0249) | VarNorm: 1.0000 (Var: 3.9464) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4671 | Clamped: 0.4671 | MagNorm: 0.1161 (AvgMag: 1.1605) | VarNorm: 1.0000 (Var: 4.4836) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5461 | Clamped: 0.5461 | MagNorm: 0.1286 (AvgMag: 1.2856) | VarNorm: 1.0000 (Var: 4.9095) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5122 | Clamped: 0.5122 | MagNorm: 0.1334 (AvgMag: 1.3340) | VarNorm: 1.0000 (Var: 5.2009) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5533 | Clamped: 0.5533 | MagNorm: 0.1429 (AvgMag: 1.4288) | VarNorm: 1.0000 (Var: 6.1141) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4366 | Clamped: 0.4366 | MagNorm: 0.1277 (AvgMag: 1.2770) | VarNorm: 1.0000 (Var: 5.1504) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4792 | Clamped: 0.4792 | MagNorm: 0.1402 (AvgMag: 1.4023) | VarNorm: 1.0000 (Var: 5.5892) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5227 | Clamped: 0.5227 | MagNorm: 0.1545 (AvgMag: 1.5454) | VarNorm: 1.0000 (Var: 6.3143) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5643 | Clamped: 0.5643 | MagNorm: 0.1650 (AvgMag: 1.6500) | VarNorm: 1.0000 (Var: 8.1173) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5095 | Clamped: 0.5095 | MagNorm: 0.1280 (AvgMag: 1.2804) | VarNorm: 1.0000 (Var: 4.3603) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5531 | Clamped: 0.5531 | MagNorm: 0.1426 (AvgMag: 1.4255) | VarNorm: 1.0000 (Var: 4.9851) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4249 | Clamped: 0.4249 | MagNorm: 0.1043 (AvgMag: 1.0433) | VarNorm: 1.0000 (Var: 3.6121) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4669 | Clamped: 0.4669 | MagNorm: 0.1156 (AvgMag: 1.1560) | VarNorm: 1.0000 (Var: 4.0494) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5128 | Clamped: 0.5128 | MagNorm: 0.1347 (AvgMag: 1.3471) | VarNorm: 1.0000 (Var: 4.8528) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5546 | Clamped: 0.5546 | MagNorm: 0.1455 (AvgMag: 1.4552) | VarNorm: 1.0000 (Var: 6.2400) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5057 | Clamped: 0.5057 | MagNorm: 0.1206 (AvgMag: 1.2059) | VarNorm: 1.0000 (Var: 4.3647) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5483 | Clamped: 0.5483 | MagNorm: 0.1329 (AvgMag: 1.3287) | VarNorm: 1.0000 (Var: 5.0547) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4200 | Clamped: 0.4200 | MagNorm: 0.0945 (AvgMag: 0.9445) | VarNorm: 1.0000 (Var: 3.9053) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4616 | Clamped: 0.4616 | MagNorm: 0.1050 (AvgMag: 1.0501) | VarNorm: 1.0000 (Var: 4.5606) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5033 | Clamped: 0.5033 | MagNorm: 0.1156 (AvgMag: 1.1562) | VarNorm: 1.0000 (Var: 5.1445) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5437 | Clamped: 0.5437 | MagNorm: 0.1238 (AvgMag: 1.2384) | VarNorm: 1.0000 (Var: 5.6396) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4857 | Clamped: 0.4857 | MagNorm: 0.0806 (AvgMag: 0.8056) | VarNorm: 1.0000 (Var: 3.3581) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5321 | Clamped: 0.5321 | MagNorm: 0.1006 (AvgMag: 1.0063) | VarNorm: 1.0000 (Var: 3.7632) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4135 | Clamped: 0.4135 | MagNorm: 0.0815 (AvgMag: 0.8146) | VarNorm: 1.0000 (Var: 3.4293) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4533 | Clamped: 0.4533 | MagNorm: 0.0885 (AvgMag: 0.8850) | VarNorm: 1.0000 (Var: 3.7586) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5333 | Clamped: 0.5333 | MagNorm: 0.1029 (AvgMag: 1.0292) | VarNorm: 1.0000 (Var: 4.2630) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5315 | Clamped: 0.5315 | MagNorm: 0.1720 (AvgMag: 1.7203) | VarNorm: 1.0000 (Var: 6.8149) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5723 | Clamped: 0.5723 | MagNorm: 0.1810 (AvgMag: 1.8101) | VarNorm: 1.0000 (Var: 7.2814) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4419 | Clamped: 0.4419 | MagNorm: 0.1384 (AvgMag: 1.3836) | VarNorm: 1.0000 (Var: 5.1492) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4859 | Clamped: 0.4859 | MagNorm: 0.1536 (AvgMag: 1.5362) | VarNorm: 1.0000 (Var: 5.9659) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5261 | Clamped: 0.5261 | MagNorm: 0.1613 (AvgMag: 1.6133) | VarNorm: 1.0000 (Var: 6.2395) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5670 | Clamped: 0.5670 | MagNorm: 0.1704 (AvgMag: 1.7038) | VarNorm: 1.0000 (Var: 7.2896) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5144 | Clamped: 0.5144 | MagNorm: 0.1378 (AvgMag: 1.3782) | VarNorm: 1.0000 (Var: 5.1245) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5555 | Clamped: 0.5555 | MagNorm: 0.1474 (AvgMag: 1.4739) | VarNorm: 1.0000 (Var: 5.4424) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4317 | Clamped: 0.4317 | MagNorm: 0.1179 (AvgMag: 1.1790) | VarNorm: 1.0000 (Var: 4.6240) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4721 | Clamped: 0.4721 | MagNorm: 0.1260 (AvgMag: 1.2599) | VarNorm: 1.0000 (Var: 5.1525) | PosF: 0.5455
[DEBUG Temp Layer 4] Raw Temp: 0.5187 | Clamped: 0.5187 | MagNorm: 0.1466 (AvgMag: 1.4657) | VarNorm: 1.0000 (Var: 6.5869) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5608 | Clamped: 0.5608 | MagNorm: 0.1579 (AvgMag: 1.5792) | VarNorm: 1.0000 (Var: 7.3906) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4977 | Clamped: 0.4977 | MagNorm: 0.1045 (AvgMag: 1.0454) | VarNorm: 1.0000 (Var: 4.3039) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5403 | Clamped: 0.5403 | MagNorm: 0.1169 (AvgMag: 1.1694) | VarNorm: 1.0000 (Var: 4.8713) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4296 | Clamped: 0.4296 | MagNorm: 0.1137 (AvgMag: 1.1365) | VarNorm: 1.0000 (Var: 4.0120) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4721 | Clamped: 0.4721 | MagNorm: 0.1261 (AvgMag: 1.2608) | VarNorm: 1.0000 (Var: 4.4842) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5579 | Clamped: 0.5579 | MagNorm: 0.1522 (AvgMag: 1.5218) | VarNorm: 1.0000 (Var: 5.9047) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4988 | Clamped: 0.4988 | MagNorm: 0.1066 (AvgMag: 1.0659) | VarNorm: 1.0000 (Var: 3.7072) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5432 | Clamped: 0.5432 | MagNorm: 0.1227 (AvgMag: 1.2272) | VarNorm: 1.0000 (Var: 4.5412) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4218 | Clamped: 0.4218 | MagNorm: 0.0982 (AvgMag: 0.9821) | VarNorm: 1.0000 (Var: 4.1259) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4598 | Clamped: 0.4598 | MagNorm: 0.1014 (AvgMag: 1.0141) | VarNorm: 1.0000 (Var: 4.2838) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5383 | Clamped: 0.5383 | MagNorm: 0.1130 (AvgMag: 1.1304) | VarNorm: 1.0000 (Var: 4.1289) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.4953 | Clamped: 0.4953 | MagNorm: 0.0998 (AvgMag: 0.9977) | VarNorm: 1.0000 (Var: 3.9003) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5377 | Clamped: 0.5377 | MagNorm: 0.1117 (AvgMag: 1.1173) | VarNorm: 1.0000 (Var: 4.2216) | PosF: 0.9091
[DEBUG Temp Layer 2] Raw Temp: 0.4450 | Clamped: 0.4450 | MagNorm: 0.1445 (AvgMag: 1.4447) | VarNorm: 1.0000 (Var: 5.5916) | PosF: 0.3636
[DEBUG Temp Layer 3] Raw Temp: 0.4873 | Clamped: 0.4873 | MagNorm: 0.1563 (AvgMag: 1.5632) | VarNorm: 1.0000 (Var: 6.0722) | PosF: 0.5455
[DEBUG Temp Layer 5] Raw Temp: 0.5694 | Clamped: 0.5694 | MagNorm: 0.1752 (AvgMag: 1.7523) | VarNorm: 1.0000 (Var: 6.6759) | PosF: 0.9091
[DEBUG Temp Layer 4] Raw Temp: 0.5146 | Clamped: 0.5146 | MagNorm: 0.1384 (AvgMag: 1.3837) | VarNorm: 1.0000 (Var: 5.4963) | PosF: 0.7273
[DEBUG Temp Layer 5] Raw Temp: 0.5560 | Clamped: 0.5560 | MagNorm: 0.1483 (AvgMag: 1.4828) | VarNorm: 1.0000 (Var: 5.8919) | PosF: 0.9091
