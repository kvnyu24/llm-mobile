INFO:hybrid_inference:Log level set to: INFO
INFO:hybrid_inference:Set pad_token to eos_token for tokenizer.
INFO:hybrid_inference:Using device: cpu
INFO:hybrid_inference:Loading model: gpt2...
INFO:hybrid_inference:Using float32 for model loading on CPU.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:HYBRID INFERENCE - REAL MODEL EXECUTION
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Model: gpt2
INFO:hybrid_inference:Device: cpu
INFO:hybrid_inference:Model dimensions: 768d, 12 layers, 12 attention heads
INFO:hybrid_inference:Prompt: "Test memory compression, layer skipping, and token pruning with pruning action"
INFO:hybrid_inference:Max New Tokens: 100
INFO:hybrid_inference:Enabled optimizations:
INFO:hybrid_inference:  - Token Pruning: True
INFO:hybrid_inference:  - Layer Opt (Skip/Compress): False
INFO:hybrid_inference:  - Memory Management (Quant): False
INFO:hybrid_inference:  - Edge-Cloud Partitioning: False (Logic not active)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Initializing Token Pruner...
INFO:hybrid_inference:Starting inference loop for 100 tokens...
`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
WARNING:token_pruning.token_pruner:Expected query length (Q) of 1 for scoring, but got 15. Using only the first query position.
INFO:token_pruning.token_pruner:Pruned 1 tokens. New seq len: 23
ERROR:hybrid_inference:Error during token pruning: tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/Users/kevinyu/Projects/llm-mobile/src/hybrid_inference.py", line 523, in run_inference
    attention_mask = pruned_outputs["attention_mask"]
                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
TypeError: tuple indices must be integers or slices, not str
INFO:hybrid_inference:
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Inference Summary (Manual Loop with Hooks)
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Total Inference Time: 23.01s
INFO:hybrid_inference:Generation Loop Time: 22.02s
INFO:hybrid_inference:Average time per generated token: 0.2201s (4.54 tokens/s)
INFO:hybrid_inference:Generated 100 new tokens
INFO:hybrid_inference:Layers skipped (decision count): 0
INFO:hybrid_inference:Tokens Pruned: 0
INFO:hybrid_inference:Layers Offloaded: 0
INFO:hybrid_inference:Final sequence length: 115
INFO:hybrid_inference:--- Generated Text ---
INFO:hybrid_inference:Prompt: Test memory compression, layer skipping, and token pruning with pruning action
INFO:hybrid_inference:Final Output: Test memory compression, layer skipping, and token pruning with pruning action.

We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is deployed and running, we will see our code run at its peak performance as we continue to push our client code to the network. In this instance, it was a huge boost since we ran our benchmark using 4GB of memory. This is a really big win for our tests because it makes it easier for us to test the process in real time.
INFO:hybrid_inference:Newly Generated Text: .

We've used a tool called D3 to help us analyze and optimize memory usage while we're on the road. Once D4 is deployed and running, we will see our code run at its peak performance as we continue to push our client code to the network. In this instance, it was a huge boost since we ran our benchmark using 4GB of memory. This is a really big win for our tests because it makes it easier for us to test the process in real time.
INFO:hybrid_inference:=====================================================
INFO:hybrid_inference:Hybrid inference script finished.
INFO:hybrid_inference:Hybrid inference script finished.
